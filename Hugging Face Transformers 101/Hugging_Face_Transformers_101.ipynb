{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hugging Face Transformers 101\n\nThe objective of this notebook is to give an extensive summary on how to use the [transformers](https://github.com/huggingface/transformers) library from [Hugging Face](https://huggingface.co/).\n\n**Useful links:**\n- [Hugging Face - Model Hub](https://huggingface.co/models)\n- [Hugging Face - Datasets](https://huggingface.co/datasets)\n","metadata":{}},{"cell_type":"markdown","source":"# Overview\n\nThe transformers library by Hugging Face provides two main ways to use pre-trained models:\n1. **Pipelines**\n    - **Purpose**: Pipelines are designed for quick, easy, and high-level access to various NLP tasks like text classification, question answering, text generation, etc. They abstract away much of the complexity involved in setting up and using models and tokenizers.\n    - **Ease of Use**: Pipelines are user-friendly and require minimal code to get started. You don’t need to worry about loading models or tokenizers separately.\n    - **Flexibility**: Pipelines are less flexible since they are designed for specific tasks and operate within the constraints of the task-specific settings.\n    - **Customization**: Limited customization options. The parameters and the way data flows through the pipeline are predefined.\n    - **Ideal For**: Beginners, rapid prototyping, and tasks where you don’t need to fine-tune or customize the behavior of the models.\n\n\n2. **AutoModel/AutoTokenizer Classes**\n    - **Purpose**: These classes are more low-level and provide greater flexibility and control over the models and tokenizers. They allow you to load any pre-trained model or tokenizer from the model hub.\n    - **Ease of Use**: Requires more setup compared to pipelines. You need to explicitly load the tokenizer and model and handle the inputs and outputs manually.\n    - **Flexibility**: Highly flexible. You can customize almost every aspect of the model’s behavior, modify the data preprocessing, and tweak how the outputs are handled.\n    - **Customization**: Extensive customization options. You can fine-tune models, change tokenization strategies, modify model architecture, or integrate with other libraries for advanced use cases.\n    - **Ideal For**: Advanced users, research, fine-tuning models, and scenarios where you need to go beyond the default behavior of the pipelines.\n","metadata":{}},{"cell_type":"markdown","source":"## Pipelines\n\nThe [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines) is the easiest and fastest way to use a pretrained model for inference. In this case the pipeline downloads and caches a default pretrained model and tokenizer for sentiment analysis\n\n| **Task**                     | **Description**                                                                                              | **Modality**    | **Pipeline identifier**                       |\n|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|\n| Text classification          | assign a label to a given sequence of text                                                                   | NLP             | pipeline(task=“sentiment-analysis”)           |\n| Text generation              | generate text given a prompt                                                                                 | NLP             | pipeline(task=“text-generation”)              |\n| Summarization                | generate a summary of a sequence of text or document                                                         | NLP             | pipeline(task=“summarization”)                |\n| Image classification         | assign a label to an image                                                                                   | Computer vision | pipeline(task=“image-classification”)         |\n| Image segmentation           | assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation) | Computer vision | pipeline(task=“image-segmentation”)           |\n| Object detection             | predict the bounding boxes and classes of objects in an image                                                | Computer vision | pipeline(task=“object-detection”)             |\n| Audio classification         | assign a label to some audio data                                                                            | Audio           | pipeline(task=“audio-classification”)         |\n| Automatic speech recognition | transcribe speech into text                                                                                  | Audio           | pipeline(task=“automatic-speech-recognition”) |\n| Visual question answering    | answer a question about the image, given an image and a question                                             | Multimodal      | pipeline(task=“vqa”)                          |\n| Document question answering  | answer a question about a document, given an image and a question                                            | Multimodal      | pipeline(task=\"document-question-answering\")  |\n| Image captioning             | generate a caption for a given image                                                                         | Multimodal      | pipeline(task=\"image-to-text\")                |\n","metadata":{}},{"cell_type":"markdown","source":"1 - `sentiment-analysis` - Vector as an Input","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\n# We can create a vector of data for the classifier\nclassifier = pipeline(\"sentiment-analysis\")\nprompts = [\"This is a very happy example :).\",\n           \"We hope you don't hate it.\"]\nresults = classifier(prompts)\nfor result in results:\n    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:31:12.285436Z","iopub.execute_input":"2024-08-16T15:31:12.285844Z","iopub.status.idle":"2024-08-16T15:31:40.975481Z","shell.execute_reply.started":"2024-08-16T15:31:12.285811Z","shell.execute_reply":"2024-08-16T15:31:40.974190Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-16 15:31:21.659547: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-16 15:31:21.659675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-16 15:31:21.839982: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nNo model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb61285a8be543888e07dd26db0e4cf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"115bf697c9714bae9122c0a7ca701c51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bd70ec6d6f44affba1d372c4dc69ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6e1b33952aa4c8dbbe101bb878b5471"}},"metadata":{}},{"name":"stdout","text":"label: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n","output_type":"stream"}]},{"cell_type":"markdown","source":"2 -  `automatic-speech-recognition` - Dataset as an Input","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Audio\n\n# Or we can give it an entire dataset\n# Lets use automatic speech recognition\nspeech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n# Lets load the dataset\ndataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n# Make sure that the data set matches the sampling rate in which the model was trained\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))\n# Print the written audio\nresult = speech_recognizer(dataset[:4][\"audio\"])\nprint([d[\"text\"] for d in result])","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:31:40.978012Z","iopub.execute_input":"2024-08-16T15:31:40.979133Z","iopub.status.idle":"2024-08-16T15:37:52.149531Z","shell.execute_reply.started":"2024-08-16T15:31:40.979082Z","shell.execute_reply":"2024-08-16T15:37:52.148229Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baaa4374dd114e70bcb0748b78d20a3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31064507d02a404d91d1908cd2d41ba6"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c70cf2815c8b404a9608fd9115b1b179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c06ea424e6ae45db87acaad43d626eb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca12c0abb4304b588e5ef9b0eb135399"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd8b0b13ed2c433ca1edec4a84b4cd62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d420f1ce9e1e41a19bea006fb0e276ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/5.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"327986250cd643a0a291078513ebd4d9"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for PolyAI/minds14 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/PolyAI/minds14.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1599e9ac8f74a0298ecb3016e44ca3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ac9a45701a4a3bac8c6105a91b5bee"}},"metadata":{}},{"name":"stdout","text":"['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I FURN A JOINA COUT']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- 3: `multilingual-uncased-sentiment` - Selectring specific model and tokenizer in the pipeline\n\nUse `AutoModelForSequenceClassification` and `AutoTokenizer` to load the pretrained model and it's associated tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Select the model name\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" # predicts the sentiment of the review as a number of stars (between 1 and 5).\n# Use AutoModelForSequenceClassification and AutoTokenizer from the named model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# Define the pipeline\nclassifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n# Run the pipeline\nclassifier(\"Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:37:52.158005Z","iopub.execute_input":"2024-08-16T15:37:52.158594Z","iopub.status.idle":"2024-08-16T15:37:59.574286Z","shell.execute_reply.started":"2024-08-16T15:37:52.158538Z","shell.execute_reply":"2024-08-16T15:37:59.573143Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb3af3838d8a4ff1b092f870b3df1505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a67c16655755428096a298a4e2322e80"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725282e8e75442dab1f8ad7c4e87a5c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80867d3c0826441d8c34dfb2e713f2dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"981d16485e9f452798c7fcc77f12e992"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[{'label': '5 stars', 'score': 0.7272652387619019}]"},"metadata":{}}]},{"cell_type":"markdown","source":"## AutoClass\nAutoClass is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path","metadata":{}},{"cell_type":"markdown","source":"### Autotokenizer\n\nTokenizer is a dictionary that returns:\n\n- `input_ids`: numerical representations of your tokens.\n- `attention_mask`: indicates which tokens should be attended to.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nencoding = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n# The tokenizer returns a dictionary containing: input_ids & attention_mask\nprint(\"encoding -> input_ids: \", encoding[\"input_ids\"])\nprint(\"encoding -> attention_mask: \", encoding[\"attention_mask\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:37:59.576329Z","iopub.execute_input":"2024-08-16T15:37:59.576822Z","iopub.status.idle":"2024-08-16T15:38:00.008291Z","shell.execute_reply.started":"2024-08-16T15:37:59.576779Z","shell.execute_reply":"2024-08-16T15:38:00.006846Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"encoding -> input_ids:  [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102]\nencoding -> attention_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokenizer can accept other inputs\npt_batch = tokenizer(\n    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n    padding=True,\n    truncation=True,\n    max_length=512,\n    return_tensors=\"pt\",\n)\nprint(\"pt_batch -> input_ids: \", pt_batch[\"input_ids\"])\nprint(\"pt_batch -> attention_mask: \", pt_batch[\"attention_mask\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:00.010242Z","iopub.execute_input":"2024-08-16T15:38:00.010611Z","iopub.status.idle":"2024-08-16T15:38:00.021739Z","shell.execute_reply.started":"2024-08-16T15:38:00.010580Z","shell.execute_reply":"2024-08-16T15:38:00.020247Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"pt_batch -> input_ids:  tensor([[  101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103,   100,\n         58263, 13299,   119,   102],\n        [  101, 11312, 18763, 10855, 11530,   112,   162, 39487, 10197,   119,\n           102,     0,     0,     0]])\npt_batch -> attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### AutoModel\nIt is important to select the desired [task](https://huggingface.co/docs/transformers/main/en/task_summary) of the model. The model outputs the final activations in the `logits` attribute, so by applying softmax function to the `logits` it is possible retrieve the probabilities","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom torch import nn\n\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\npt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Pass the preprocessed batch of inputs directly to the model by unpack the dictionary with **\npt_outputs = pt_model(**pt_batch)\n\npt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\nprint(pt_predictions)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:00.023282Z","iopub.execute_input":"2024-08-16T15:38:00.023694Z","iopub.status.idle":"2024-08-16T15:38:02.613253Z","shell.execute_reply.started":"2024-08-16T15:38:00.023649Z","shell.execute_reply":"2024-08-16T15:38:02.608700Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"All 🤗 Transformers models output the tensors before the final activation function (like softmax) because the final activation function is often fused with the loss.\n","metadata":{}},{"cell_type":"markdown","source":"### Save a Model\n\n1. `save_pretrained` to save the model","metadata":{}},{"cell_type":"code","source":"# Once the model is fine-tuned you can save it\npt_save_directory = \"./pt_save_pretrained\"\ntokenizer.save_pretrained(pt_save_directory)\npt_model.save_pretrained(pt_save_directory)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:02.619807Z","iopub.execute_input":"2024-08-16T15:38:02.622780Z","iopub.status.idle":"2024-08-16T15:38:06.524775Z","shell.execute_reply.started":"2024-08-16T15:38:02.622613Z","shell.execute_reply":"2024-08-16T15:38:06.521873Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"2. `from_pretrained` to load the model","metadata":{}},{"cell_type":"code","source":"# Then you can reload it\npt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:06.529205Z","iopub.execute_input":"2024-08-16T15:38:06.529911Z","iopub.status.idle":"2024-08-16T15:38:07.575604Z","shell.execute_reply.started":"2024-08-16T15:38:06.529866Z","shell.execute_reply":"2024-08-16T15:38:07.574112Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Custom Model Builds\nYou can modify the model's configuration class to change how a model is built\n\n1. `Autoconfig` to store the pretrained model config, and yoy select the attribute that you want to change ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig\nmy_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", n_heads=12)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:07.580799Z","iopub.execute_input":"2024-08-16T15:38:07.581317Z","iopub.status.idle":"2024-08-16T15:38:07.819819Z","shell.execute_reply.started":"2024-08-16T15:38:07.581269Z","shell.execute_reply":"2024-08-16T15:38:07.818503Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c8d3acef3ba4e73943aa3a1031e3125"}},"metadata":{}}]},{"cell_type":"markdown","source":"2. `AutoModel` to create a new model with the custom config","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel\nmy_model = AutoModel.from_config(my_config)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:07.821384Z","iopub.execute_input":"2024-08-16T15:38:07.821871Z","iopub.status.idle":"2024-08-16T15:38:09.696151Z","shell.execute_reply.started":"2024-08-16T15:38:07.821830Z","shell.execute_reply":"2024-08-16T15:38:09.694854Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Trainer (Pytorch)\n\n- All models are a standard `torch.nn.Module` so you can use them in any typical training loop \n- Transformers provides a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for PyTorch\n     - Contains the basic training loop and adds additional functionality Idistributed training, mixed precision, etc)\n     - You can also write your own training loop","metadata":{}},{"cell_type":"markdown","source":"1.  **Model** (`PreTrainedModel` or `torch.nnModule`)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:09.697757Z","iopub.execute_input":"2024-08-16T15:38:09.698205Z","iopub.status.idle":"2024-08-16T15:38:11.911641Z","shell.execute_reply.started":"2024-08-16T15:38:09.698162Z","shell.execute_reply":"2024-08-16T15:38:11.910348Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3550f232ad7418fbe858c1eb61a50a5"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"2. **Training Arguments** (hyperparametes)","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"path/to/save/folder/\",\n                                  learning_rate=2e-5,\n                                  per_device_train_batch_size=8,\n                                  per_device_eval_batch_size=8,\n                                  num_train_epochs=2,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:11.913299Z","iopub.execute_input":"2024-08-16T15:38:11.913682Z","iopub.status.idle":"2024-08-16T15:38:11.936848Z","shell.execute_reply.started":"2024-08-16T15:38:11.913650Z","shell.execute_reply":"2024-08-16T15:38:11.935481Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"3. **Preprocessing class** (tokenizer, image processor, feature extractor, or processor)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:11.938531Z","iopub.execute_input":"2024-08-16T15:38:11.938974Z","iopub.status.idle":"2024-08-16T15:38:13.606262Z","shell.execute_reply.started":"2024-08-16T15:38:11.938936Z","shell.execute_reply":"2024-08-16T15:38:13.604940Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd60760ee83d40188966de1138e7db67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c402fcda7ba2404dbe6b99ce911fa315"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82896774d54042318f61827ad1cbe592"}},"metadata":{}}]},{"cell_type":"markdown","source":"4. **Load a dataset**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"rotten_tomatoes\")  # doctest: +IGNORE_RESULT","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:13.608076Z","iopub.execute_input":"2024-08-16T15:38:13.608746Z","iopub.status.idle":"2024-08-16T15:38:21.744778Z","shell.execute_reply.started":"2024-08-16T15:38:13.608699Z","shell.execute_reply":"2024-08-16T15:38:21.739176Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e88d3df4b5db48c48ee3533053fd324e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/699k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"898bde856a2a4171b6680e41a30d93f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/90.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3c9d963e4ae404588feb6f5a82e95bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/92.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9efa3b04ec104cc182a8578f58aa27fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"946a4aebefe44536a1fe9daba7075df5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e55b9fe8a56d4073aaf03f647fd2ac76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85cce8bfafa41d7a4c72b4ee0930eb8"}},"metadata":{}}]},{"cell_type":"markdown","source":"5. **Tokenize the dataset**","metadata":{}},{"cell_type":"code","source":"# Create a function to tokenize the dataset\ndef tokenize_dataset(dataset):\n    return tokenizer(dataset[\"text\"])\n\n# Apply it to the entire dataset\ndataset = dataset.map(tokenize_dataset, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:21.753705Z","iopub.execute_input":"2024-08-16T15:38:21.759957Z","iopub.status.idle":"2024-08-16T15:38:25.447674Z","shell.execute_reply.started":"2024-08-16T15:38:21.759618Z","shell.execute_reply":"2024-08-16T15:38:25.445889Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8530 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2e29a8ff124f93861d82edf09d6fcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb17584c207e49668f71e800f932c9ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80b8008e6fe448e084c68cb56f749b4f"}},"metadata":{}}]},{"cell_type":"markdown","source":"6 - **Data Collator with Padding** (to create a batch of examples from your dataset)","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:25.449706Z","iopub.execute_input":"2024-08-16T15:38:25.450238Z","iopub.status.idle":"2024-08-16T15:38:25.456814Z","shell.execute_reply.started":"2024-08-16T15:38:25.450184Z","shell.execute_reply":"2024-08-16T15:38:25.455487Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)  # doctest: +SKIP","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:25.458560Z","iopub.execute_input":"2024-08-16T15:38:25.459372Z","iopub.status.idle":"2024-08-16T15:38:27.658717Z","shell.execute_reply.started":"2024-08-16T15:38:25.459320Z","shell.execute_reply":"2024-08-16T15:38:27.657510Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-16T15:38:27.660716Z","iopub.execute_input":"2024-08-16T15:38:27.661573Z","iopub.status.idle":"2024-08-16T15:42:34.264254Z","shell.execute_reply.started":"2024-08-16T15:38:27.661520Z","shell.execute_reply":"2024-08-16T15:42:34.259852Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2196\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2194\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m   2195\u001b[0m grad_norm: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_on_start:\n\u001b[1;32m   2199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py:461\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    460\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py:508\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 508\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:842\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:776\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mtermwarn(\n\u001b[1;32m    770\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    771\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    772\u001b[0m             repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    773\u001b[0m         )\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWANDB_PROJECT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuggingface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;66;03m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1195\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1195\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1180\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1179\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1180\u001b[0m     \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wi\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:300\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     settings\u001b[38;5;241m.\u001b[39mupdate(init_settings, source\u001b[38;5;241m=\u001b[39mSource\u001b[38;5;241m.\u001b[39mINIT)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39m_offline \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39m_noop:\n\u001b[0;32m--> 300\u001b[0m     \u001b[43mwandb_login\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43manonymous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manonymous\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_warning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_silent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_entity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# apply updated global state after login was handled\u001b[39;00m\n\u001b[1;32m    309\u001b[0m wl \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39msetup()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:346\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logged_in\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[0;32m--> 346\u001b[0m     \u001b[43mwlogin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wlogin\u001b[38;5;241m.\u001b[39m_key \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:273\u001b[0m, in \u001b[0;36m_WandbLogin.prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_api_key\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     key, status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m ApiKeyStatus\u001b[38;5;241m.\u001b[39mNOTTY:\n\u001b[1;32m    275\u001b[0m         directive \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb login [your_api_key]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_cli_only_mode\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb.login(key=[your_api_key])\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:252\u001b[0m, in \u001b[0;36m_WandbLogin._prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mapikey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_api_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mno_offline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mno_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# invalid key provided, try again\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermerror(e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/apikey.py:164\u001b[0m, in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    158\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermlog(\n\u001b[1;32m    159\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogging into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. (Learn how to deploy a W&B server locally: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwburls\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_server\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[1;32m    161\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermlog(\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can find your API key in your browser here: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/authorize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m     )\n\u001b[0;32m--> 164\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43minput_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_ask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    165\u001b[0m write_key(settings, key, api\u001b[38;5;241m=\u001b[39mapi)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m key  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/click/termui.py:164\u001b[0m, in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/click/termui.py:147\u001b[0m, in \u001b[0;36mprompt.<locals>.prompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hide_input:\n\u001b[1;32m    146\u001b[0m     echo(\u001b[38;5;28;01mNone\u001b[39;00m, err\u001b[38;5;241m=\u001b[39merr)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Abort() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mAbort\u001b[0m: "],"ename":"Abort","evalue":"","output_type":"error"},{"output_type":"stream","name":"stdin","text":"  ········································\n"}]},{"cell_type":"markdown","source":"# Examples of Different Tasks","metadata":{}},{"cell_type":"markdown","source":"## Sequence Classification","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n\nclasses = [\"not paraphrase\", \"is paraphrase\"]\n\nsequence_0 = \"The sun sets behind the mountains, painting the sky in shades of orange and pink\"\nsequence_1 = \"The city buzzed with life as the night markets opened, filling the streets with vibrant colors and delicious aromas\"\nsequence_2 = \"The sky turns a blend of orange and pink as the sun dips below the mountain peaks.\"\n\n# The tokenizer will automatically add any model specific separators, as well as compute the attention masks.\nparaphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\nnot_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n\nparaphrase_classification_logits = model(**paraphrase).logits\nnot_paraphrase_classification_logits = model(**not_paraphrase).logits\n\nparaphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\nnot_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n\nprint(\"This should be paraphrase\")\nfor i in range(len(classes)):\n    print(f\"-> {classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n    \nprint(\"This should not be paraphrase\")\nfor i in range(len(classes)):\n    print(f\"-> {classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T01:48:55.545740Z","iopub.execute_input":"2024-08-17T01:48:55.546179Z","iopub.status.idle":"2024-08-17T01:49:06.660226Z","shell.execute_reply.started":"2024-08-17T01:48:55.546145Z","shell.execute_reply":"2024-08-17T01:49:06.658830Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"269f291db7a545d4aa22b22ff9daf3dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e732a5e47e49ad95206b7fb7f6659f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e30bc1f1d18488b8d2894be3c2b305c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb4e864324074ed88f258dafdb4e1327"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b45be2805549528cd3fc2cf7c147d5"}},"metadata":{}},{"name":"stdout","text":"This should be paraphrase\n-> not paraphrase: 8%\n-> is paraphrase: 92%\nThis should not be paraphrase\n-> not paraphrase: 94%\n-> is paraphrase: 6%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Extractive Queue Answering","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport torch\n\n# Load pre-trained tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\n# Define a new context text\ntext = r\"\"\"\nPython is a versatile programming language that supports multiple programming paradigms, including procedural,\nobject-oriented, and functional programming. Python's extensive standard library and its dynamic nature make it\na popular choice for both beginners and experienced developers. It is widely used in web development, data science,\nautomation, and scientific computing.\n\"\"\"\n\n# Define a new set of questions based on the context\nquestions = [\n    \"What programming paradigms does Python support?\",\n    \"Why is Python popular among developers?\",\n    \"In what fields is Python widely used?\",\n]\n\n# Iterate over each question and find the answer\nfor question in questions:\n    # Tokenize the question and context together\n    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n\n    # Pass the tokenized inputs to the model\n    outputs = model(**inputs)\n    answer_start_scores = outputs.start_logits\n    answer_end_scores = outputs.end_logits\n\n    # Identify the start and end of the answer\n    answer_start = torch.argmax(answer_start_scores)\n    answer_end = torch.argmax(answer_end_scores) + 1\n\n    # Convert the token IDs to a string\n    answer = tokenizer.convert_tokens_to_string(\n        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n    )\n\n    # Print the question and the corresponding answer\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T01:55:46.892865Z","iopub.execute_input":"2024-08-17T01:55:46.893293Z","iopub.status.idle":"2024-08-17T01:55:57.670113Z","shell.execute_reply.started":"2024-08-17T01:55:46.893261Z","shell.execute_reply":"2024-08-17T01:55:57.668947Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d28ec1bccfa54f1ebb9c4d3917b5bc33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea6afaa2b07b4969bca4cce6f26c5555"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f453674cb34facb9c40a841b3c0edb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5c16df61bb049419f814f9ec9b8fee2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784febc493794986a18db0174e9f72ed"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Question: What programming paradigms does Python support?\nAnswer: procedural, object - oriented, and functional programming\nQuestion: Why is Python popular among developers?\nAnswer: extensive standard library and its dynamic nature\nQuestion: In what fields is Python widely used?\nAnswer: web development, data science, automation, and scientific computing\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Masked Language Modeling","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Causal Language Modeling","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Generation","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Named Entity Recognition","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summarization","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Translation","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Audio Classification","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Automatic Speech Recognition","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Classification","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}