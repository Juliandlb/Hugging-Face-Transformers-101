{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hugging Face Transformers 101\n\nThe objective of this notebook is to give an extensive summary on how to use the [transformers](https://github.com/huggingface/transformers) library from [Hugging Face](https://huggingface.co/).\n\n**Useful links:**\n- [Hugging Face - Model Hub](https://huggingface.co/models)\n- [Hugging Face - Datasets](https://huggingface.co/datasets)\n","metadata":{}},{"cell_type":"code","source":"pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:48:31.225578Z","iopub.execute_input":"2024-08-17T02:48:31.225915Z","iopub.status.idle":"2024-08-17T02:48:56.450691Z","shell.execute_reply.started":"2024-08-17T02:48:31.225886Z","shell.execute_reply":"2024-08-17T02:48:56.449700Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting transformers\n  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed transformers-4.44.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Overview\n\nThe transformers library by Hugging Face provides two main ways to use pre-trained models:\n1. **Pipelines**\n    - **Purpose**: Pipelines are designed for quick, easy, and high-level access to various NLP tasks like text classification, question answering, text generation, etc. They abstract away much of the complexity involved in setting up and using models and tokenizers.\n    - **Ease of Use**: Pipelines are user-friendly and require minimal code to get started. You don‚Äôt need to worry about loading models or tokenizers separately.\n    - **Flexibility**: Pipelines are less flexible since they are designed for specific tasks and operate within the constraints of the task-specific settings.\n    - **Customization**: Limited customization options. The parameters and the way data flows through the pipeline are predefined.\n    - **Ideal For**: Beginners, rapid prototyping, and tasks where you don‚Äôt need to fine-tune or customize the behavior of the models.\n\n\n2. **AutoModel/AutoTokenizer Classes**\n    - **Purpose**: These classes are more low-level and provide greater flexibility and control over the models and tokenizers. They allow you to load any pre-trained model or tokenizer from the model hub.\n    - **Ease of Use**: Requires more setup compared to pipelines. You need to explicitly load the tokenizer and model and handle the inputs and outputs manually.\n    - **Flexibility**: Highly flexible. You can customize almost every aspect of the model‚Äôs behavior, modify the data preprocessing, and tweak how the outputs are handled.\n    - **Customization**: Extensive customization options. You can fine-tune models, change tokenization strategies, modify model architecture, or integrate with other libraries for advanced use cases.\n    - **Ideal For**: Advanced users, research, fine-tuning models, and scenarios where you need to go beyond the default behavior of the pipelines.\n","metadata":{}},{"cell_type":"markdown","source":"## Pipelines\n\nThe [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines) is the easiest and fastest way to use a pretrained model for inference. In this case the pipeline downloads and caches a default pretrained model and tokenizer for sentiment analysis\n\n| **Task**                     | **Description**                                                                                              | **Modality**    | **Pipeline identifier**                       |\n|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|\n| Text classification          | assign a label to a given sequence of text                                                                   | NLP             | pipeline(task=‚Äúsentiment-analysis‚Äù)           |\n| Text generation              | generate text given a prompt                                                                                 | NLP             | pipeline(task=‚Äútext-generation‚Äù)              |\n| Summarization                | generate a summary of a sequence of text or document                                                         | NLP             | pipeline(task=‚Äúsummarization‚Äù)                |\n| Image classification         | assign a label to an image                                                                                   | Computer vision | pipeline(task=‚Äúimage-classification‚Äù)         |\n| Image segmentation           | assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation) | Computer vision | pipeline(task=‚Äúimage-segmentation‚Äù)           |\n| Object detection             | predict the bounding boxes and classes of objects in an image                                                | Computer vision | pipeline(task=‚Äúobject-detection‚Äù)             |\n| Audio classification         | assign a label to some audio data                                                                            | Audio           | pipeline(task=‚Äúaudio-classification‚Äù)         |\n| Automatic speech recognition | transcribe speech into text                                                                                  | Audio           | pipeline(task=‚Äúautomatic-speech-recognition‚Äù) |\n| Visual question answering    | answer a question about the image, given an image and a question                                             | Multimodal      | pipeline(task=‚Äúvqa‚Äù)                          |\n| Document question answering  | answer a question about a document, given an image and a question                                            | Multimodal      | pipeline(task=\"document-question-answering\")  |\n| Image captioning             | generate a caption for a given image                                                                         | Multimodal      | pipeline(task=\"image-to-text\")                |\n","metadata":{}},{"cell_type":"markdown","source":"1 - `sentiment-analysis` - Vector as an Input","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\n# We can create a vector of data for the classifier\nclassifier = pipeline(\"sentiment-analysis\")\nprompts = [\"This is a very happy example :).\",\n           \"We hope you don't hate it.\"]\nresults = classifier(prompts)\nfor result in results:\n    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:48:56.452788Z","iopub.execute_input":"2024-08-17T02:48:56.453106Z","iopub.status.idle":"2024-08-17T02:49:18.375087Z","shell.execute_reply.started":"2024-08-17T02:48:56.453076Z","shell.execute_reply":"2024-08-17T02:49:18.374209Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-17 02:49:03.420800: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-17 02:49:03.420905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-17 02:49:03.582274: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nNo model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ab11a4c22314c569a1bd1f0a4d2a6c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2154f723c984540bc1ee8f3ea2a37bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d796da9e637549f59eb7e5f55ff0efc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef05ced0f074399bf4290b0b8e436a2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"label: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n","output_type":"stream"}]},{"cell_type":"markdown","source":"2 -  `automatic-speech-recognition` - Dataset as an Input","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Audio\n\n# Or we can give it an entire dataset\n# Lets use automatic speech recognition\nspeech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n# Lets load the dataset\ndataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n# Make sure that the data set matches the sampling rate in which the model was trained\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))\n# Print the written audio\nresult = speech_recognizer(dataset[:4][\"audio\"])\nprint([d[\"text\"] for d in result])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:49:18.376389Z","iopub.execute_input":"2024-08-17T02:49:18.376954Z","iopub.status.idle":"2024-08-17T02:50:10.203423Z","shell.execute_reply.started":"2024-08-17T02:49:18.376928Z","shell.execute_reply":"2024-08-17T02:50:10.202442Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12da3e2607ed427c9a5afc185474e3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e805faff52f94997b250a2221ca60d84"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f8b495c4324a82a0ac96b098a8ed72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a00dd6ccd40641188dd85726c06147ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edd3937de1bd46a7be1b8ab99cbc3362"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a2b89b81854d9483fdb82b722f25e0"}},"metadata":{}},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0d8fd52271c4b09ac056ea3d43fe6e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/5.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9439ae6f00cf47c4bb0c89cd1306f87b"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for PolyAI/minds14 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/PolyAI/minds14.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"214c4c29a6cf40258f054cad52f79c76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121927b8557947b7bb4a0b37f28f5263"}},"metadata":{}},{"name":"stdout","text":"['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I FURN A JOINA COUT']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- 3: `multilingual-uncased-sentiment` - Selectring specific model and tokenizer in the pipeline\n\nUse `AutoModelForSequenceClassification` and `AutoTokenizer` to load the pretrained model and it's associated tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Select the model name\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" # predicts the sentiment of the review as a number of stars (between 1 and 5).\n# Use AutoModelForSequenceClassification and AutoTokenizer from the named model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# Define the pipeline\nclassifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n# Run the pipeline\nclassifier(\"Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:10.206280Z","iopub.execute_input":"2024-08-17T02:50:10.207202Z","iopub.status.idle":"2024-08-17T02:50:14.570051Z","shell.execute_reply.started":"2024-08-17T02:50:10.207130Z","shell.execute_reply":"2024-08-17T02:50:14.569121Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7991443622bf41a58890e0940740b399"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78986df721fb49eb8ccfef722a70366e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cf636c637df4c82bf31513bcad81631"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7db8e5cca04481480fa336e23fd4494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62135d53d84f475895791ef9c21e5b75"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[{'label': '5 stars', 'score': 0.7272651791572571}]"},"metadata":{}}]},{"cell_type":"markdown","source":"## AutoClass\nAutoClass is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path","metadata":{}},{"cell_type":"markdown","source":"### Autotokenizer\n\nTokenizer is a dictionary that returns:\n\n- `input_ids`: numerical representations of your tokens.\n- `attention_mask`: indicates which tokens should be attended to.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nencoding = tokenizer(\"We are very happy to show you the ü§ó Transformers library.\")\n# The tokenizer returns a dictionary containing: input_ids & attention_mask\nprint(\"encoding -> input_ids: \", encoding[\"input_ids\"])\nprint(\"encoding -> attention_mask: \", encoding[\"attention_mask\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:14.571301Z","iopub.execute_input":"2024-08-17T02:50:14.571624Z","iopub.status.idle":"2024-08-17T02:50:14.892064Z","shell.execute_reply.started":"2024-08-17T02:50:14.571598Z","shell.execute_reply":"2024-08-17T02:50:14.891127Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"encoding -> input_ids:  [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102]\nencoding -> attention_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokenizer can accept other inputs\npt_batch = tokenizer(\n    [\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"],\n    padding=True,\n    truncation=True,\n    max_length=512,\n    return_tensors=\"pt\",\n)\nprint(\"pt_batch -> input_ids: \", pt_batch[\"input_ids\"])\nprint(\"pt_batch -> attention_mask: \", pt_batch[\"attention_mask\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:14.893331Z","iopub.execute_input":"2024-08-17T02:50:14.893631Z","iopub.status.idle":"2024-08-17T02:50:14.900951Z","shell.execute_reply.started":"2024-08-17T02:50:14.893606Z","shell.execute_reply":"2024-08-17T02:50:14.899980Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"pt_batch -> input_ids:  tensor([[  101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103,   100,\n         58263, 13299,   119,   102],\n        [  101, 11312, 18763, 10855, 11530,   112,   162, 39487, 10197,   119,\n           102,     0,     0,     0]])\npt_batch -> attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### AutoModel\nIt is important to select the desired [task](https://huggingface.co/docs/transformers/main/en/task_summary) of the model. The model outputs the final activations in the `logits` attribute, so by applying softmax function to the `logits` it is possible retrieve the probabilities","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom torch import nn\n\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\npt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Pass the preprocessed batch of inputs directly to the model by unpack the dictionary with **\npt_outputs = pt_model(**pt_batch)\n\npt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\nprint(pt_predictions)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:14.902069Z","iopub.execute_input":"2024-08-17T02:50:14.902355Z","iopub.status.idle":"2024-08-17T02:50:15.655398Z","shell.execute_reply.started":"2024-08-17T02:50:14.902332Z","shell.execute_reply":"2024-08-17T02:50:15.654482Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"All ü§ó Transformers models output the tensors before the final activation function (like softmax) because the final activation function is often fused with the loss.\n","metadata":{}},{"cell_type":"markdown","source":"### Save a Model\n\n1. `save_pretrained` to save the model","metadata":{}},{"cell_type":"code","source":"# Once the model is fine-tuned you can save it\npt_save_directory = \"./pt_save_pretrained\"\ntokenizer.save_pretrained(pt_save_directory)\npt_model.save_pretrained(pt_save_directory)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:15.656563Z","iopub.execute_input":"2024-08-17T02:50:15.656851Z","iopub.status.idle":"2024-08-17T02:50:16.966595Z","shell.execute_reply.started":"2024-08-17T02:50:15.656826Z","shell.execute_reply":"2024-08-17T02:50:16.965782Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"2. `from_pretrained` to load the model","metadata":{}},{"cell_type":"code","source":"# Then you can reload it\npt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:16.967727Z","iopub.execute_input":"2024-08-17T02:50:16.968016Z","iopub.status.idle":"2024-08-17T02:50:17.056758Z","shell.execute_reply.started":"2024-08-17T02:50:16.967992Z","shell.execute_reply":"2024-08-17T02:50:17.055848Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Custom Model Builds\nYou can modify the model's configuration class to change how a model is built\n\n1. `Autoconfig` to store the pretrained model config, and yoy select the attribute that you want to change ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig\nmy_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", n_heads=12)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:17.060087Z","iopub.execute_input":"2024-08-17T02:50:17.060376Z","iopub.status.idle":"2024-08-17T02:50:17.936727Z","shell.execute_reply.started":"2024-08-17T02:50:17.060352Z","shell.execute_reply":"2024-08-17T02:50:17.935873Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"232e25630e44422abc5fff6b9a04d58a"}},"metadata":{}}]},{"cell_type":"markdown","source":"2. `AutoModel` to create a new model with the custom config","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel\nmy_model = AutoModel.from_config(my_config)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:17.937868Z","iopub.execute_input":"2024-08-17T02:50:17.938144Z","iopub.status.idle":"2024-08-17T02:50:19.187774Z","shell.execute_reply.started":"2024-08-17T02:50:17.938118Z","shell.execute_reply":"2024-08-17T02:50:19.186979Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Trainer (Pytorch)\n\n- All models are a standard `torch.nn.Module` so you can use them in any typical training loop \n- Transformers provides a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for PyTorch\n     - Contains the basic training loop and adds additional functionality Idistributed training, mixed precision, etc)\n     - You can also write your own training loop","metadata":{}},{"cell_type":"markdown","source":"1.  **Model** (`PreTrainedModel` or `torch.nnModule`)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:19.189048Z","iopub.execute_input":"2024-08-17T02:50:19.189649Z","iopub.status.idle":"2024-08-17T02:50:20.398486Z","shell.execute_reply.started":"2024-08-17T02:50:19.189612Z","shell.execute_reply":"2024-08-17T02:50:20.397787Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d0f4b9fd424c49a18e50520aae5c04"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"2. **Training Arguments** (hyperparametes)","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"path/to/save/folder/\",\n                                  learning_rate=2e-5,\n                                  per_device_train_batch_size=8,\n                                  per_device_eval_batch_size=8,\n                                  num_train_epochs=2,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:20.399597Z","iopub.execute_input":"2024-08-17T02:50:20.399882Z","iopub.status.idle":"2024-08-17T02:50:20.480356Z","shell.execute_reply.started":"2024-08-17T02:50:20.399857Z","shell.execute_reply":"2024-08-17T02:50:20.479406Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"3. **Preprocessing class** (tokenizer, image processor, feature extractor, or processor)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:20.481545Z","iopub.execute_input":"2024-08-17T02:50:20.481835Z","iopub.status.idle":"2024-08-17T02:50:21.785949Z","shell.execute_reply.started":"2024-08-17T02:50:20.481810Z","shell.execute_reply":"2024-08-17T02:50:21.785018Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4c65c5fe8df4deca8780d8b3d8ca503"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9974141d3edc46df9608917a7382036e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c46708b3cb749c6ac16c07bea620725"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"4. **Load a dataset**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"rotten_tomatoes\")  # doctest: +IGNORE_RESULT","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:21.787245Z","iopub.execute_input":"2024-08-17T02:50:21.787506Z","iopub.status.idle":"2024-08-17T02:50:26.255625Z","shell.execute_reply.started":"2024-08-17T02:50:21.787484Z","shell.execute_reply":"2024-08-17T02:50:26.254695Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d85cec83c9be442791d99d60b2b1cd01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/699k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27f414bb2b04a6aa18b832e8eb78154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/90.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1d9de180b7147dea1acd3e2ac6b2065"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/92.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21e45ffe95db48049a9e84142a8db145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c621e8014a744decbf7cdf2d4902e2e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a4873089324c5ca100894a7d0ca3f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9270947eda5047298d46715fed9383fb"}},"metadata":{}}]},{"cell_type":"markdown","source":"5. **Tokenize the dataset**","metadata":{}},{"cell_type":"code","source":"# Create a function to tokenize the dataset\ndef tokenize_dataset(dataset):\n    return tokenizer(dataset[\"text\"])\n\n# Apply it to the entire dataset\ndataset = dataset.map(tokenize_dataset, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:26.256877Z","iopub.execute_input":"2024-08-17T02:50:26.257242Z","iopub.status.idle":"2024-08-17T02:50:27.626043Z","shell.execute_reply.started":"2024-08-17T02:50:26.257208Z","shell.execute_reply":"2024-08-17T02:50:27.625134Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8530 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"411e324f87f5403e9230f57d3a0e37be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be7ce87e1cb84baea39763bf2484933a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eae988071f4422eb1972ca1f76cac9a"}},"metadata":{}}]},{"cell_type":"markdown","source":"6 - **Data Collator with Padding** (to create a batch of examples from your dataset)","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:27.627200Z","iopub.execute_input":"2024-08-17T02:50:27.627654Z","iopub.status.idle":"2024-08-17T02:50:27.634484Z","shell.execute_reply.started":"2024-08-17T02:50:27.627622Z","shell.execute_reply":"2024-08-17T02:50:27.632238Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)  # doctest: +SKIP","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:27.636496Z","iopub.execute_input":"2024-08-17T02:50:27.637030Z","iopub.status.idle":"2024-08-17T02:50:29.532488Z","shell.execute_reply.started":"2024-08-17T02:50:27.636989Z","shell.execute_reply":"2024-08-17T02:50:29.531415Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:50:29.533758Z","iopub.execute_input":"2024-08-17T02:50:29.534014Z","iopub.status.idle":"2024-08-17T02:52:56.313490Z","shell.execute_reply.started":"2024-08-17T02:50:29.533990Z","shell.execute_reply":"2024-08-17T02:52:56.312594Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240817_025047-4oo4nfu8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/juliandlb-pontificia-universidad-cat-lica-de-chile/huggingface/runs/4oo4nfu8' target=\"_blank\">path/to/save/folder/</a></strong> to <a href='https://wandb.ai/juliandlb-pontificia-universidad-cat-lica-de-chile/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/juliandlb-pontificia-universidad-cat-lica-de-chile/huggingface' target=\"_blank\">https://wandb.ai/juliandlb-pontificia-universidad-cat-lica-de-chile/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/juliandlb-pontificia-universidad-cat-lica-de-chile/huggingface/runs/4oo4nfu8' target=\"_blank\">https://wandb.ai/juliandlb-pontificia-universidad-cat-lica-de-chile/huggingface/runs/4oo4nfu8</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1068' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1068/1068 01:50, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.417400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.249600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1068, training_loss=0.3269694476538383, metrics={'train_runtime': 146.3854, 'train_samples_per_second': 116.542, 'train_steps_per_second': 7.296, 'total_flos': 215637261882480.0, 'train_loss': 0.3269694476538383, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Examples of Different Tasks","metadata":{}},{"cell_type":"markdown","source":"## Sequence Classification","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n\nclasses = [\"not paraphrase\", \"is paraphrase\"]\n\nsequence_0 = \"The sun sets behind the mountains, painting the sky in shades of orange and pink\"\nsequence_1 = \"The city buzzed with life as the night markets opened, filling the streets with vibrant colors and delicious aromas\"\nsequence_2 = \"The sky turns a blend of orange and pink as the sun dips below the mountain peaks.\"\n\n# The tokenizer will automatically add any model specific separators, as well as compute the attention masks.\nparaphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\nnot_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n\nparaphrase_classification_logits = model(**paraphrase).logits\nnot_paraphrase_classification_logits = model(**not_paraphrase).logits\n\nparaphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\nnot_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n\nprint(\"This should be paraphrase\")\nfor i in range(len(classes)):\n    print(f\"-> {classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n    \nprint(\"This should not be paraphrase\")\nfor i in range(len(classes)):\n    print(f\"-> {classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:52:56.314957Z","iopub.execute_input":"2024-08-17T02:52:56.315283Z","iopub.status.idle":"2024-08-17T02:52:59.593078Z","shell.execute_reply.started":"2024-08-17T02:52:56.315258Z","shell.execute_reply":"2024-08-17T02:52:59.591892Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b34ff78614f459688232456ac1f81d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9da71ab31093439e8d1fed799931e4ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29089b97e6c846c19bf5a1541df89627"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d3adf5aaa741fb857578421ebb775d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d598b36aaa1e488db4111246453812ac"}},"metadata":{}},{"name":"stdout","text":"This should be paraphrase\n-> not paraphrase: 8%\n-> is paraphrase: 92%\nThis should not be paraphrase\n-> not paraphrase: 94%\n-> is paraphrase: 6%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Extractive Queue Answering","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport torch\n\n# Load pre-trained tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\n# Define a new context text\ntext = r\"\"\"\nPython is a versatile programming language that supports multiple programming paradigms, including procedural,\nobject-oriented, and functional programming. Python's extensive standard library and its dynamic nature make it\na popular choice for both beginners and experienced developers. It is widely used in web development, data science,\nautomation, and scientific computing.\n\"\"\"\n\n# Define a new set of questions based on the context\nquestions = [\n    \"What programming paradigms does Python support?\",\n    \"Why is Python popular among developers?\",\n    \"In what fields is Python widely used?\",\n]\n\n# Iterate over each question and find the answer\nfor question in questions:\n    # Tokenize the question and context together\n    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n\n    # Pass the tokenized inputs to the model\n    outputs = model(**inputs)\n    answer_start_scores = outputs.start_logits\n    answer_end_scores = outputs.end_logits\n\n    # Identify the start and end of the answer\n    answer_start = torch.argmax(answer_start_scores)\n    answer_end = torch.argmax(answer_end_scores) + 1\n\n    # Convert the token IDs to a string\n    answer = tokenizer.convert_tokens_to_string(\n        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n    )\n\n    # Print the question and the corresponding answer\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:52:59.594395Z","iopub.execute_input":"2024-08-17T02:52:59.594727Z","iopub.status.idle":"2024-08-17T02:53:07.792008Z","shell.execute_reply.started":"2024-08-17T02:52:59.594698Z","shell.execute_reply":"2024-08-17T02:53:07.790224Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b4a9ac90d7c4f29bce3b444bc1b8592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"056d5d8fbe0e4f30aea0f5e2f85925ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539ee13238d24b3f847991804b68f63a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dcd12f28b3a497ca75bb90ed8ddc97c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55771e1567934543a0986bff8a01b296"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Question: What programming paradigms does Python support?\nAnswer: procedural, object - oriented, and functional programming\nQuestion: Why is Python popular among developers?\nAnswer: extensive standard library and its dynamic nature\nQuestion: In what fields is Python widely used?\nAnswer: web development, data science, automation, and scientific computing\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Masked Language Modeling","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForMaskedLM, AutoTokenizer\nimport torch\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n\n# Define a new sequence with a masked token\nsequence = (\n    \"Artificial intelligence is a rapidly growing field that has the potential to \"\n    f\"revolutionize {tokenizer.mask_token} in many industries.\"\n)\n\n# Tokenize the input sequence\ninputs = tokenizer(sequence, return_tensors=\"pt\")\nmask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n\n# Get the logits from the model\ntoken_logits = model(**inputs).logits\nmask_token_logits = token_logits[0, mask_token_index, :]\n\n# Get the top 5 token predictions for the masked token\ntop_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n\n# Replace the masked token with each of the top 5 predictions and print the results\nfor token in top_5_tokens:\n    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:53:07.793605Z","iopub.execute_input":"2024-08-17T02:53:07.793976Z","iopub.status.idle":"2024-08-17T02:53:10.674849Z","shell.execute_reply.started":"2024-08-17T02:53:07.793942Z","shell.execute_reply":"2024-08-17T02:53:10.673926Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea64487dd7fb423d83685ac4bf78cba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c67317be1824a379fa6ca32db513dfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"911faa2b2ee848f0a80762be4b97bffb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"137588910a48457095b656d5cdc5bc06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c755460025364e809ef56762bbde28b4"}},"metadata":{}},{"name":"stdout","text":"Artificial intelligence is a rapidly growing field that has the potential to revolutionize computing in many industries.\nArtificial intelligence is a rapidly growing field that has the potential to revolutionize technology in many industries.\nArtificial intelligence is a rapidly growing field that has the potential to revolutionize intelligence in many industries.\nArtificial intelligence is a rapidly growing field that has the potential to revolutionize innovation in many industries.\nArtificial intelligence is a rapidly growing field that has the potential to revolutionize science in many industries.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Causal Language Modeling","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom torch import nn\n\n# Initialize the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Input sequence\nsequence = \"Hugging Face is based in DUMBO, New York City, and\"\n\n# Tokenize the input sequence\ninputs = tokenizer(sequence, return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"]\n\n# Get logits of the last hidden state\nnext_token_logits = model(**inputs).logits[:, -1, :]\n\n# Implement top-k and top-p filtering manually\ndef top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n    assert logits.dim() == 1  # Ensure logits have the expected shape\n\n    # Top-k filtering\n    if top_k > 0:\n        top_k = min(top_k, logits.size(-1))  # Ensure top_k is within the vocabulary size\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    # Nucleus (top-p) filtering\n    if top_p > 0.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold\n        sorted_indices_to_remove = cumulative_probs > top_p\n        # Shift the indices to the right to keep also the first token above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = filter_value\n\n    return logits\n\n# Apply the filtering function\nfiltered_next_token_logits = top_k_top_p_filtering(next_token_logits.squeeze(), top_k=50, top_p=1.0)\n\n# Sample from the filtered logits\nprobs = nn.functional.softmax(filtered_next_token_logits, dim=-1)\nnext_token = torch.multinomial(probs, num_samples=1)\n\n# Concatenate the sampled token to the input sequence\ngenerated = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n\n# Decode the generated sequence into text\nresulting_string = tokenizer.decode(generated.tolist()[0])\nprint(resulting_string)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:53:10.676630Z","iopub.execute_input":"2024-08-17T02:53:10.677421Z","iopub.status.idle":"2024-08-17T02:53:15.007752Z","shell.execute_reply.started":"2024-08-17T02:53:10.677384Z","shell.execute_reply":"2024-08-17T02:53:15.006925Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49910813010f4d7fb7df5a99e8a55ac4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a251fb399fde40959891bc2437c242dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"603bc0c7e6b24ce382fab9b79c32df67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad9557307889415aae17c1e3d0c411a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb8a40d48ab42baab77cd7bcb20c6cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c387534556d44c6b114faf0892dadf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6458a6ffde74430a2fd665d46467ec7"}},"metadata":{}},{"name":"stdout","text":"Hugging Face is based in DUMBO, New York City, and is\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Text Generation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"xlnet-base-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n\n# PADDING_TEXT with a story\nPADDING_TEXT = \"\"\"In the year 2075, humanity has established colonies on Mars. The red planet is now home to\na thriving community of scientists, engineers, and families. The story begins with the discovery of an\nancient Martian artifact buried beneath the surface. Dr. Elara Quinn, a leading archaeologist, is called\nto investigate the find. As she examines the artifact, she experiences strange visions and begins to\nunravel the secrets of a long-lost Martian civilization. The discovery sets off a chain of events that\ncould change the course of human history. <eod> </s> <eos>\"\"\"\n\n# Prompt\nprompt = \"The team of astronauts prepared for their mission, knowing that \"\n\n# Tokenize the combined PADDING_TEXT and prompt\ninputs = tokenizer(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n\n# Calculate the length of the prompt\nprompt_length = len(tokenizer.decode(inputs[0]))\n\n# Generate the continuation of the text\noutputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\ngenerated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1:]\n\n# Print the generated text\nprint(generated)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:53:15.008968Z","iopub.execute_input":"2024-08-17T02:53:15.009259Z","iopub.status.idle":"2024-08-17T02:53:40.801122Z","shell.execute_reply.started":"2024-08-17T02:53:15.009233Z","shell.execute_reply":"2024-08-17T02:53:40.800185Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4903fe35e6f4302bd7d842e19b0259c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e44c1451bbea46a29febc8276ddf7ad9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ced6533cb2b4f5c8c2c4cebe2fffb90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0bd3bd869e146bba935d2c3e0bab0d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cce8707904e489385b9fda0fd6ad89b"}},"metadata":{}},{"name":"stderr","text":"This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (-1). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n","output_type":"stream"},{"name":"stdout","text":"The team of astronauts prepared for their mission, knowing that they would be a leader, the last individual to be in the service of the United States will be able to do so in the first few months of the service. With a mission planned for the next 12 weeks, the team of astronauts have to consider what they can do for the mission. They will also need to plan for the long-term success of the mission, making sure that the company can sustain itself and their employees. They will also have to prepare for the very long-term commitment of the mission.<eop> The first three weeks of the mission have been planned for six months by NASA and will include several weeks of a\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Named Entity Recognition","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\n# Load the model and tokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Sequence for Named Entity Recognition (NER)\nsequence = (\n    \"NASA's Jet Propulsion Laboratory (JPL) is a research and development center \"\n    \"located in Pasadena, California. JPL is responsible for several high-profile space missions.\"\n)\n\n# Tokenize the input sequence\ninputs = tokenizer(sequence, return_tensors=\"pt\")\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\n# Get model outputs\noutputs = model(**inputs).logits\npredictions = torch.argmax(outputs, dim=2)\n\n# Print tokens and their predicted labels\nfor token, prediction in zip(tokens, predictions[0].numpy()):\n    print((token, model.config.id2label[prediction]))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:53:40.802455Z","iopub.execute_input":"2024-08-17T02:53:40.802992Z","iopub.status.idle":"2024-08-17T02:53:56.375125Z","shell.execute_reply.started":"2024-08-17T02:53:40.802963Z","shell.execute_reply":"2024-08-17T02:53:56.374191Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e92d51527174a58b9ab3247e6664578"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be56d1178ed74a439398008e4a334103"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4df4fbbc93bf42fa85118470ab306dfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91fdc5debd7d4a66b1bfce94edffb347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec17a35255a6404887e57b21f296ecbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82b157394a2c421182090e48046761b8"}},"metadata":{}},{"name":"stdout","text":"('[CLS]', 'O')\n('NASA', 'I-ORG')\n(\"'\", 'O')\n('s', 'O')\n('Jet', 'I-ORG')\n('Pro', 'I-ORG')\n('##pulsion', 'I-ORG')\n('Laboratory', 'I-ORG')\n('(', 'O')\n('JP', 'I-ORG')\n('##L', 'I-ORG')\n(')', 'O')\n('is', 'O')\n('a', 'O')\n('research', 'O')\n('and', 'O')\n('development', 'O')\n('center', 'O')\n('located', 'O')\n('in', 'O')\n('Pasadena', 'I-LOC')\n(',', 'O')\n('California', 'I-LOC')\n('.', 'O')\n('JP', 'I-ORG')\n('##L', 'I-ORG')\n('is', 'O')\n('responsible', 'O')\n('for', 'O')\n('several', 'O')\n('high', 'O')\n('-', 'O')\n('profile', 'O')\n('space', 'O')\n('missions', 'O')\n('.', 'O')\n('[SEP]', 'O')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Summarization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# ARTICLE\nARTICLE = \"\"\"In 2023, scientists at the European Space Agency (ESA) achieved a significant milestone in space exploration with the successful deployment of the Euclid space telescope.\nThis telescope is designed to investigate the mysterious dark energy and dark matter that make up most of the universe's mass-energy content. Euclid will map the geometry of the dark universe\nby measuring the shapes and distances of billions of galaxies. The mission aims to improve our understanding of the universe's expansion and the forces driving it. The Euclid mission represents\na collaborative effort involving international space agencies and research institutions. The telescope is equipped with state-of-the-art instruments to capture detailed images of distant galaxies\nand analyze their distribution. By studying the large-scale structure of the universe, scientists hope to unlock new insights into fundamental questions about cosmic evolution and the nature of dark\nenergy. The launch of Euclid marks a major advancement in our quest to explore the cosmos and understand the underlying forces shaping our universe.\"\"\"\n\n# Load the model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n\n# Prepare the input for summarization\ninputs = tokenizer(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512, truncation=True)\n\n# Generate the summary\noutputs = model.generate(\n    inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n)\n\n# Decode and print the summary\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:53:56.376572Z","iopub.execute_input":"2024-08-17T02:53:56.377125Z","iopub.status.idle":"2024-08-17T02:54:06.919042Z","shell.execute_reply.started":"2024-08-17T02:53:56.377072Z","shell.execute_reply":"2024-08-17T02:54:06.917886Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be847eecf8049d39ce507aac1003ad1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b910924bb1c947138cff35f280a399ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a9d4b7ae7cd4e5a8c66b3de4551bed1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"821894a30c8c4a8a9d5790a15ac22dc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb7ebfeb91714320862f29f29c004248"}},"metadata":{}},{"name":"stdout","text":"the telescope is designed to investigate the mysterious dark energy and dark matter that make up most of the universe's mass-energy content. the launch of Euclid marks a major advancement in our quest to explore the cosmos and understand the forces shaping our universe.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Translation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load the model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n\n# New text for translation\ninputs = tokenizer(\n    \"translate English to German: The quick brown fox jumps over the lazy dog\",\n    return_tensors=\"pt\",\n)\noutputs = model.generate(inputs[\"input_ids\"], max_length=40, num_beams=4, early_stopping=True)\n\n# Print the translated text\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:54:06.920546Z","iopub.execute_input":"2024-08-17T02:54:06.920826Z","iopub.status.idle":"2024-08-17T02:54:09.790378Z","shell.execute_reply.started":"2024-08-17T02:54:06.920801Z","shell.execute_reply":"2024-08-17T02:54:09.789478Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Der schnelle braune Fuchs springt √ºber den faulen Hund\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Image Classification","metadata":{}},{"cell_type":"code","source":"from transformers import AutoImageProcessor, AutoModelForImageClassification\nimport torch\nfrom datasets import load_dataset\n\n# Load dataset and select an image\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\n# Load processor and model\nfeature_extractor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n\n# Preprocess the image\ninputs = feature_extractor(image, return_tensors=\"pt\")\n\n# Perform inference\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# Get and print the predicted label\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T02:54:09.795391Z","iopub.execute_input":"2024-08-17T02:54:09.795759Z","iopub.status.idle":"2024-08-17T02:54:16.879693Z","shell.execute_reply.started":"2024-08-17T02:54:09.795732Z","shell.execute_reply":"2024-08-17T02:54:16.878733Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907483005aeb4a94911a958b3d250fa0"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for huggingface/cats-image contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/huggingface/cats-image.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/173k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9178ed76c6e6476a9387bd95f00d3a22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e920f3ea61a94de0a55c317881d5d1d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b311a4f621c4a5da39b49b7809a9974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f95167e028c44bc8d7627111ff2b6d9"}},"metadata":{}},{"name":"stderr","text":"Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9f6c1d5a01a4baea51ddbc11fd8cc7c"}},"metadata":{}},{"name":"stdout","text":"Egyptian cat\n","output_type":"stream"}]}]}