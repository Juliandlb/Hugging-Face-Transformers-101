{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hugging Face Transformers 101\n\nThe objective of this notebook is to give an extensive summary on how to use the [transformers](https://github.com/huggingface/transformers) library from [Hugging Face](https://huggingface.co/).","metadata":{}},{"cell_type":"markdown","source":"# Pipelines\n\nThe [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines) is the easiest and fastest way to use a pretrained model for inference. In this case the pipeline downloads and caches a default pretrained model and tokenizer for sentiment analysis\n\n| **Task**                     | **Description**                                                                                              | **Modality**    | **Pipeline identifier**                       |\n|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|\n| Text classification          | assign a label to a given sequence of text                                                                   | NLP             | pipeline(task=‚Äúsentiment-analysis‚Äù)           |\n| Text generation              | generate text given a prompt                                                                                 | NLP             | pipeline(task=‚Äútext-generation‚Äù)              |\n| Summarization                | generate a summary of a sequence of text or document                                                         | NLP             | pipeline(task=‚Äúsummarization‚Äù)                |\n| Image classification         | assign a label to an image                                                                                   | Computer vision | pipeline(task=‚Äúimage-classification‚Äù)         |\n| Image segmentation           | assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation) | Computer vision | pipeline(task=‚Äúimage-segmentation‚Äù)           |\n| Object detection             | predict the bounding boxes and classes of objects in an image                                                | Computer vision | pipeline(task=‚Äúobject-detection‚Äù)             |\n| Audio classification         | assign a label to some audio data                                                                            | Audio           | pipeline(task=‚Äúaudio-classification‚Äù)         |\n| Automatic speech recognition | transcribe speech into text                                                                                  | Audio           | pipeline(task=‚Äúautomatic-speech-recognition‚Äù) |\n| Visual question answering    | answer a question about the image, given an image and a question                                             | Multimodal      | pipeline(task=‚Äúvqa‚Äù)                          |\n| Document question answering  | answer a question about a document, given an image and a question                                            | Multimodal      | pipeline(task=\"document-question-answering\")  |\n| Image captioning             | generate a caption for a given image                                                                         | Multimodal      | pipeline(task=\"image-to-text\")                |\n","metadata":{}},{"cell_type":"markdown","source":"## Example 1: Sentiment Analysis - Vector as an Input","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\n# We can create a vector of data for the classifier\nclassifier = pipeline(\"sentiment-analysis\")\nprompts = [\"This is a very happy example :).\",\n           \"We hope you don't hate it.\"]\nresults = classifier(prompts)\nfor result in results:\n    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T02:07:41.806515Z","iopub.execute_input":"2024-08-14T02:07:41.807591Z","iopub.status.idle":"2024-08-14T02:07:42.223954Z","shell.execute_reply.started":"2024-08-14T02:07:41.807549Z","shell.execute_reply":"2024-08-14T02:07:42.222569Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"name":"stdout","text":"label: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Example 2: Automatic Speech Recognition - Dataset as an Input","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Audio\n\n# Or we can give it an entire dataset\n# Lets use automatic speech recognition\nspeech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n# Lets load the dataset\ndataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n# Make sure that the data set matches the sampling rate in which the model was trained\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))\n# Print the written audio\nresult = speech_recognizer(dataset[:4][\"audio\"])\nprint([d[\"text\"] for d in result])","metadata":{"execution":{"iopub.status.busy":"2024-08-14T02:02:21.535998Z","iopub.execute_input":"2024-08-14T02:02:21.536440Z","iopub.status.idle":"2024-08-14T02:03:13.983858Z","shell.execute_reply.started":"2024-08-14T02:02:21.536405Z","shell.execute_reply":"2024-08-14T02:03:13.982467Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a23d9bf08c8642da9f3d1af809cf2464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/5.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23d859316a0f4df5a6cc34da0ef063cb"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for PolyAI/minds14 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/PolyAI/minds14.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ec7b947b854c48a4d91c84ac35ac1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4214a17a634ea99a08388b04953c3c"}},"metadata":{}},{"name":"stdout","text":"['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I FURN A JOINA COUT']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Example 3: Sentiment Analysis - Selectring specific model and tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nclassifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\nclassifier(\"Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T02:10:09.333797Z","iopub.execute_input":"2024-08-14T02:10:09.334268Z","iopub.status.idle":"2024-08-14T02:10:14.517945Z","shell.execute_reply.started":"2024-08-14T02:10:09.334233Z","shell.execute_reply":"2024-08-14T02:10:14.516706Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab07aa2732e147c49e7f18e9949d39d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22d49f288dfb42269e4e0b939bc36648"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5b94e97bcd84667b2a1b32e7ff4c054"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779f20fdf69744aa888c4c886c3d4b6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba5e266d7a17481480139df79f76b0f2"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[{'label': '5 stars', 'score': 0.7272652387619019}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# AutoClass","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}